{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[random.uniform(-1,1) for _ in range(5)] for _ in range(5)])\n",
    "y_hat1 = torch.tensor([[random.uniform(-1,1) for _ in range(5)] for _ in range(5)])\n",
    "y_hat2 = torch.tensor([[random.uniform(-1,1) for _ in range(5)] for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9502,  0.3249,  0.1046, -0.5830,  0.5382],\n",
       "        [-0.5722, -0.7821,  0.2562,  0.3731, -0.2288],\n",
       "        [-0.6229, -0.4932, -0.4104,  0.4971, -0.3963],\n",
       "        [ 0.2224,  0.0449,  0.9748, -0.0567,  0.0360],\n",
       "        [-0.4666, -0.2912,  0.7198,  0.8341,  0.5134]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8606,  0.5009, -0.0752,  0.0440, -0.5374],\n",
       "        [ 0.8447,  0.1780, -0.3972,  0.5538,  0.2526],\n",
       "        [ 0.3745,  0.3091, -0.5996, -0.3342, -0.9431],\n",
       "        [ 0.0583, -0.2235, -0.2145, -0.1232,  0.5406],\n",
       "        [ 0.0488,  0.5234, -0.1976, -0.8691, -0.2349]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6374,  0.6311,  0.5968, -0.8559,  0.2754],\n",
       "        [-0.5760,  0.5544, -0.1516,  0.6198, -0.6075],\n",
       "        [ 0.5725,  0.9771,  0.0730, -0.0070,  0.6653],\n",
       "        [-0.4892, -0.3212,  0.8014, -0.6269, -0.5846],\n",
       "        [-0.0944, -0.0203, -0.6810, -0.9409, -0.4528]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\deepl\\lib\\site-packages\\torch\\nn\\functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1 : -0.09833110868930817\n",
      "loss 2 : -0.0968465507030487\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.KLDivLoss()\n",
    "loss1 = criterion(y,y_hat1)\n",
    "loss2 = criterion(y,y_hat2)\n",
    "print('loss 1 : {}'.format(loss1))\n",
    "print('loss 2 : {}'.format(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1 : 5.215019226074219\n",
      "loss 2 : 2.4658799171447754\n"
     ]
    }
   ],
   "source": [
    "criterion = DiceLoss()\n",
    "loss1 = criterion(y,y_hat1)\n",
    "loss2 = criterion(y,y_hat2)\n",
    "print('loss 1 : {}'.format(loss1))\n",
    "print('loss 2 : {}'.format(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = torch.randn(100, 128, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8677, -0.8928,  0.2799,  ..., -1.4881,  1.0726, -1.1159],\n",
       "        [ 2.6298,  2.0846,  1.6118,  ..., -0.5865,  0.2716, -0.3736],\n",
       "        [-0.7671,  0.4672, -0.3905,  ...,  0.6298, -0.5888, -0.0464],\n",
       "        ...,\n",
       "        [ 0.0485, -0.9030, -1.1079,  ...,  0.5373, -0.9645,  0.2447],\n",
       "        [-1.0917, -0.7731,  2.3227,  ...,  0.8215,  0.7577, -0.8133],\n",
       "        [ 0.4787, -0.2534,  1.3955,  ..., -0.4374, -1.5205,  0.6001]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import *\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from tqdm.notebook import tqdm as notetqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'skimai/spanberta-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification(MODEL_NAME,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (MLP): Linear(in_features=768, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'Embedding' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b659c7652fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda\\envs\\deepl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 779\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'Embedding' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaLayer(\n",
       "  (attention): RobertaAttention(\n",
       "    (self): RobertaSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): RobertaSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): RobertaIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): RobertaOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].attention.self.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0013,  0.0397,  0.0352,  ...,  0.0087, -0.0279,  0.0172],\n",
       "         [ 0.0298,  0.0487, -0.0277,  ..., -0.0174,  0.0145,  0.0212],\n",
       "         [ 0.0028, -0.0338, -0.0392,  ...,  0.0202, -0.0394,  0.0125],\n",
       "         ...,\n",
       "         [-0.0269,  0.0373, -0.0087,  ..., -0.0152, -0.0086, -0.0107],\n",
       "         [-0.0309, -0.0189, -0.0469,  ...,  0.0455, -0.0070,  0.0312],\n",
       "         [ 0.0212,  0.0141,  0.0004,  ..., -0.0121, -0.0122,  0.0239]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.1704e-02,  4.6958e-03,  1.1399e-02, -1.5390e-02,  8.7822e-03,\n",
       "         -9.4673e-03,  3.5090e-03, -1.5868e-02,  2.0370e-02,  7.9437e-03,\n",
       "          5.1930e-04, -1.3330e-02,  7.3201e-03, -3.3357e-04,  4.9465e-03,\n",
       "         -1.1159e-03,  8.3455e-03, -1.4708e-02, -1.4520e-02, -7.2312e-03,\n",
       "          3.1131e-02, -1.7900e-03,  1.9223e-02, -6.0176e-03,  3.2437e-03,\n",
       "         -5.6747e-04,  5.4753e-03,  1.9259e-02, -1.6914e-03,  1.5346e-04,\n",
       "          2.0017e-02,  2.0931e-02,  1.8996e-02,  1.2319e-02, -7.9357e-03,\n",
       "         -1.0377e-02, -6.5964e-03,  9.5876e-04, -1.3218e-02, -2.7646e-03,\n",
       "         -6.5513e-03,  4.0136e-03, -1.8685e-02,  4.4694e-03, -3.6710e-03,\n",
       "          3.2679e-02, -1.5939e-02,  8.9166e-03,  1.0504e-02, -5.7850e-03,\n",
       "          3.1113e-03,  7.8085e-03,  8.3444e-04,  7.1441e-03,  4.4329e-03,\n",
       "         -1.2916e-02, -1.3430e-02,  4.7878e-03,  6.1500e-03,  5.9635e-03,\n",
       "         -8.1869e-04, -1.5464e-02,  2.6231e-02,  2.8744e-02, -1.0680e-02,\n",
       "         -2.1412e-02, -1.1366e-02, -1.4677e-02,  5.3485e-04,  7.5348e-03,\n",
       "          2.2001e-03,  1.6048e-02, -1.5944e-03, -1.2400e-02, -5.2393e-03,\n",
       "         -1.1461e-02, -8.1281e-03, -1.0978e-02,  2.7076e-03,  5.5634e-03,\n",
       "         -5.4081e-03,  4.7218e-03, -1.7729e-04,  4.3832e-03, -2.3204e-03,\n",
       "          1.2851e-02,  8.9393e-03, -1.1089e-02, -2.1869e-02,  3.4141e-03,\n",
       "          2.9220e-03,  5.7872e-05, -1.0557e-02,  7.1160e-03,  2.0528e-03,\n",
       "         -6.9181e-03, -1.0388e-02,  1.4104e-02, -6.2005e-03,  7.6226e-03,\n",
       "         -4.2066e-03,  1.3655e-02, -3.0665e-03, -1.2753e-02,  3.3020e-03,\n",
       "         -2.4146e-03, -2.3950e-03,  3.8736e-03,  3.7177e-03,  1.3932e-02,\n",
       "          1.4740e-03,  5.1143e-03, -3.4970e-03, -2.1218e-02,  2.2595e-02,\n",
       "         -2.9545e-02, -6.2233e-03,  6.4154e-03,  8.5841e-03,  7.0342e-03,\n",
       "          2.5378e-03, -6.4930e-03, -8.7867e-03, -1.2319e-02, -1.2258e-02,\n",
       "         -2.3771e-03,  7.0786e-03,  6.5257e-03, -9.9528e-04, -1.9974e-03,\n",
       "         -1.4264e-03,  3.3238e-03,  4.3982e-03,  4.3909e-03, -4.2017e-03,\n",
       "          5.5148e-03,  5.9731e-03,  9.7881e-03, -1.5116e-02, -1.2378e-02,\n",
       "          3.8670e-03, -1.4267e-02, -1.2377e-02, -2.2678e-03, -1.5831e-03,\n",
       "          7.2502e-03, -1.3355e-03,  3.0727e-03, -1.2111e-02,  8.5071e-03,\n",
       "          5.2123e-03,  4.4933e-03,  8.2500e-03,  8.4240e-03,  4.0763e-03,\n",
       "          1.9245e-03, -1.3350e-05, -1.4159e-02,  1.4670e-02,  9.5059e-03,\n",
       "         -3.8646e-03, -6.5452e-03, -6.5686e-03,  7.2167e-03, -6.4974e-03,\n",
       "         -9.5345e-03, -1.2958e-02,  3.8044e-03, -4.5713e-03, -1.3593e-03,\n",
       "          1.3097e-02,  5.5750e-03,  7.0447e-03, -7.3253e-03, -3.8804e-03,\n",
       "         -6.3558e-03,  3.4602e-03,  5.2077e-03,  2.0593e-02, -3.1781e-03,\n",
       "          5.1697e-03,  6.5463e-04,  1.6263e-03,  1.7977e-02,  2.5637e-03,\n",
       "         -6.4568e-03, -1.2245e-02,  1.6332e-03,  5.4310e-03,  1.2513e-02,\n",
       "          6.4797e-03,  7.0564e-03,  7.7781e-03, -1.6268e-02, -6.9870e-04,\n",
       "         -1.3077e-02, -2.3211e-03,  1.9244e-02, -8.8054e-03, -2.4115e-03,\n",
       "          1.9343e-02,  1.0088e-02, -3.4265e-02, -1.6852e-02,  1.0982e-02,\n",
       "          1.7664e-02,  4.5280e-02,  7.3517e-03, -1.3109e-02, -9.1992e-03,\n",
       "          7.8080e-03, -1.1370e-02, -1.2578e-02, -2.7331e-03, -7.1611e-03,\n",
       "         -3.0857e-02, -2.4271e-03,  2.0708e-02, -1.9499e-02, -1.4412e-02,\n",
       "          1.0055e-02,  6.3126e-03, -1.3887e-02,  6.8291e-03, -4.2713e-03,\n",
       "         -6.3399e-03, -9.0744e-03, -1.0854e-02,  3.6929e-03,  1.1839e-02,\n",
       "         -1.8084e-02,  6.8387e-04, -1.5293e-02, -1.6931e-02, -1.3466e-02,\n",
       "         -1.3727e-02, -3.0785e-03,  1.9252e-03, -2.1313e-02, -1.5389e-02,\n",
       "          2.5519e-02,  1.1905e-03,  2.8027e-02, -1.4116e-02,  3.6466e-02,\n",
       "          4.2045e-03, -1.5066e-02,  1.1949e-02,  5.2215e-03,  7.6781e-03,\n",
       "         -6.4152e-03,  2.7219e-02, -6.2731e-03, -1.0914e-03,  5.2947e-03,\n",
       "         -1.6294e-02, -2.0323e-02, -2.2661e-03,  1.7437e-03, -1.7307e-02,\n",
       "         -9.0138e-05,  2.6483e-02,  2.9135e-02,  2.1018e-02, -1.3239e-02,\n",
       "          9.0791e-03, -1.4097e-03,  1.8688e-02,  9.4157e-03,  2.1075e-02,\n",
       "          1.2962e-02, -4.6515e-02, -9.5627e-03,  2.8765e-03,  1.3457e-02,\n",
       "         -3.6464e-02, -1.8825e-02,  6.4866e-03, -3.1665e-02, -1.4936e-02,\n",
       "         -3.0738e-02,  4.0926e-03,  2.7383e-02,  4.0374e-03,  3.3079e-03,\n",
       "         -2.1978e-02, -5.6619e-03, -8.4390e-03, -1.7615e-02, -1.0679e-02,\n",
       "         -7.9283e-03,  1.2427e-02,  4.1547e-04, -9.8142e-03, -2.3197e-02,\n",
       "         -1.0743e-02, -1.0359e-02, -7.0717e-03,  2.2893e-02,  5.6939e-03,\n",
       "          2.5284e-02, -1.2853e-02,  1.0968e-03, -2.2948e-02,  1.2860e-02,\n",
       "          3.5702e-04,  1.0036e-02, -3.7317e-03,  1.5981e-02,  1.3932e-02,\n",
       "          2.8771e-03, -1.0367e-02, -7.5459e-04, -3.6977e-02,  1.0112e-02,\n",
       "          2.5556e-03, -3.6449e-03,  2.0001e-02, -4.9587e-03, -1.1655e-02,\n",
       "          9.9362e-03, -3.1455e-03, -5.4524e-03,  1.1869e-02, -1.1413e-03,\n",
       "          4.5307e-03,  2.1576e-02, -2.1040e-02,  8.1291e-03, -4.6273e-03,\n",
       "         -1.0848e-02,  5.0157e-03,  9.1791e-04,  2.3385e-02, -1.2091e-02,\n",
       "          1.5649e-02,  2.8795e-03,  1.4211e-02,  2.7050e-02,  9.7304e-03,\n",
       "          4.7985e-03, -1.9465e-03, -3.4462e-03,  7.6617e-03, -1.1369e-02,\n",
       "         -3.2691e-03, -1.0412e-02,  1.0523e-02,  7.7187e-03,  1.7073e-03,\n",
       "         -3.5746e-03, -2.3172e-02,  1.6698e-03,  5.9648e-03,  1.0978e-02,\n",
       "         -1.6843e-02,  1.1872e-02, -1.7561e-03,  8.5599e-03, -9.9213e-03,\n",
       "          1.4509e-02,  1.9936e-02, -1.7543e-02, -1.8749e-02, -1.7830e-02,\n",
       "         -9.2227e-04, -1.4943e-03, -9.3112e-03,  9.1755e-03, -6.4720e-03,\n",
       "         -4.5547e-03,  1.6898e-02, -9.2485e-03, -1.1454e-02, -2.3205e-04,\n",
       "          1.7026e-02, -7.9791e-03, -1.0008e-02,  1.9713e-03,  6.3681e-05,\n",
       "          1.6027e-02,  2.0213e-02, -6.9612e-03,  2.9180e-02,  8.9658e-03,\n",
       "         -1.4397e-02,  1.3896e-02, -1.0527e-02, -1.0745e-02,  1.7630e-02,\n",
       "         -3.3508e-03, -5.7486e-03,  2.7905e-03, -1.2260e-02,  7.7429e-03,\n",
       "          6.0110e-03,  2.4218e-02,  9.1520e-03,  1.4783e-02,  6.9926e-03,\n",
       "         -2.1026e-02, -9.8197e-03,  1.5252e-02, -5.3691e-03,  6.0871e-03,\n",
       "          1.8332e-02,  1.8533e-03, -8.8895e-03, -9.2332e-03,  8.8912e-03,\n",
       "         -1.7126e-02,  1.2622e-02, -1.4377e-03, -3.2183e-03, -5.5001e-03,\n",
       "          9.5102e-03,  8.1358e-03, -6.4817e-03,  9.5108e-03,  1.0328e-02,\n",
       "         -4.1851e-03,  7.7409e-03,  2.5591e-02, -1.3122e-02,  1.0043e-02,\n",
       "         -3.0063e-02,  1.0280e-03,  2.3124e-02,  2.4886e-03, -1.1993e-02,\n",
       "         -8.4012e-03, -1.0743e-03, -1.1928e-02,  4.8022e-03,  2.9683e-03,\n",
       "         -2.2462e-03,  7.3115e-03,  3.0452e-02, -1.9432e-02,  1.1148e-03,\n",
       "          2.8882e-02, -5.9867e-03, -4.7034e-03,  1.1495e-02, -2.1360e-02,\n",
       "          3.2514e-03,  1.6724e-02, -1.0680e-02,  6.2637e-03,  1.4737e-04,\n",
       "          1.0815e-02, -1.9566e-02, -2.0051e-03, -8.0923e-03, -4.5439e-03,\n",
       "          3.2282e-03,  6.0987e-03,  9.0512e-04, -2.0641e-02,  2.3312e-03,\n",
       "          1.8736e-02,  5.0833e-03,  2.1102e-02, -2.7791e-03,  9.2463e-03,\n",
       "         -2.3682e-03,  7.2530e-03, -4.9217e-03, -3.9925e-03, -7.4148e-03,\n",
       "         -7.3405e-03,  4.4040e-03, -1.9687e-02, -1.7492e-03, -6.5835e-03,\n",
       "          1.5271e-02,  7.4031e-03,  1.5185e-02,  5.5675e-04,  1.2612e-02,\n",
       "         -2.1819e-02, -3.5206e-02,  3.5200e-03,  2.9776e-03,  2.2237e-04,\n",
       "         -7.3767e-04,  1.5170e-03, -1.1775e-02,  2.1643e-02, -2.9200e-03,\n",
       "          4.0833e-03, -9.5811e-04,  1.1343e-02, -1.7204e-03, -1.0698e-02,\n",
       "         -3.6326e-03,  1.5995e-02,  2.8680e-03,  7.4593e-03,  6.0320e-03,\n",
       "         -5.8078e-03,  2.1742e-02, -6.4670e-04,  1.2317e-02,  3.1760e-02,\n",
       "          2.0127e-02, -2.4503e-02,  1.3996e-02, -1.8948e-02, -1.4654e-02,\n",
       "         -3.5328e-03,  2.6958e-02, -2.1846e-02,  2.0174e-02,  6.8842e-03,\n",
       "         -7.5876e-03,  3.8035e-02, -3.4526e-03,  6.8576e-03,  6.1655e-03,\n",
       "         -2.6156e-03,  1.4745e-02, -1.8526e-02, -2.1758e-02,  2.6549e-02,\n",
       "         -1.6126e-02,  2.8745e-02, -1.5494e-02,  5.1672e-03, -3.3962e-02,\n",
       "         -1.1154e-02, -8.2266e-03,  5.9121e-03, -2.9244e-02, -9.5894e-03,\n",
       "          1.8262e-02,  1.3367e-03,  1.8305e-02, -7.1176e-03,  1.9087e-02,\n",
       "         -2.5841e-02,  1.5855e-02,  8.1518e-03, -1.4160e-02, -1.8450e-02,\n",
       "         -1.5531e-02, -1.6445e-02,  8.7103e-03, -2.9489e-02, -1.9381e-02,\n",
       "         -5.7440e-03, -1.8298e-02, -4.8866e-03, -2.5353e-02, -2.2137e-03,\n",
       "         -1.3964e-02, -1.3144e-03,  2.0926e-02, -1.0926e-02, -4.4164e-02,\n",
       "          6.1520e-03,  4.2206e-04,  1.2005e-02,  1.7856e-02,  5.3104e-03,\n",
       "          2.9998e-02,  1.9450e-02, -8.3739e-03,  5.4183e-03,  3.8926e-02,\n",
       "          1.7782e-03, -2.2038e-02,  3.0241e-02,  5.9782e-03, -3.4829e-02,\n",
       "          3.4936e-02,  7.7932e-04,  6.3329e-03, -1.3938e-03, -1.3716e-02,\n",
       "          9.3509e-03, -2.6291e-03,  3.4078e-03,  1.5033e-03, -5.2910e-03,\n",
       "          1.1637e-02, -9.7548e-04,  4.0656e-03,  1.0706e-02, -1.1427e-02,\n",
       "         -7.9132e-03, -7.6883e-03, -9.8664e-03,  1.3805e-02, -1.0686e-03,\n",
       "         -2.2351e-02,  9.6778e-03, -8.2101e-03, -1.5541e-03, -3.4965e-03,\n",
       "          8.7211e-05, -5.8704e-03, -1.1858e-02,  8.7545e-04,  7.2480e-03,\n",
       "         -6.2542e-03, -1.5722e-02, -1.3502e-02, -2.1707e-03,  1.0388e-02,\n",
       "         -3.1523e-02, -1.4941e-04, -8.3965e-03,  6.6483e-03,  2.2021e-02,\n",
       "         -1.4092e-02, -9.6132e-03,  3.6669e-03,  4.2705e-03,  9.9002e-03,\n",
       "          1.0664e-02, -1.3640e-02, -1.3107e-03, -3.5645e-03, -1.1419e-02,\n",
       "         -1.3776e-02,  2.8514e-03, -6.3460e-03, -8.4525e-03,  3.0211e-03,\n",
       "          5.2496e-04,  1.1522e-03, -7.0781e-03, -1.3070e-02, -6.0888e-04,\n",
       "          5.9496e-03, -9.3788e-03, -6.6772e-03, -1.5979e-03, -8.2723e-03,\n",
       "          4.9595e-03,  1.3557e-03,  7.7101e-03,  1.9758e-02,  1.4086e-02,\n",
       "          1.7104e-03, -2.9098e-03,  1.4914e-02,  7.6355e-03,  6.5801e-03,\n",
       "          1.7375e-02, -2.1854e-02, -9.9910e-03,  8.3866e-03, -4.6605e-03,\n",
       "          1.6624e-03, -2.3128e-02,  2.2345e-03, -7.9295e-03,  4.6856e-03,\n",
       "         -2.7913e-02,  5.8016e-03, -1.5261e-02, -1.1863e-04, -1.8107e-03,\n",
       "          4.5126e-03,  1.8644e-03,  1.9409e-02, -1.4192e-02,  1.0123e-02,\n",
       "          1.9645e-02, -1.5300e-03, -2.0673e-03,  3.0070e-03, -1.5359e-02,\n",
       "          3.1796e-03, -6.8070e-03,  4.1986e-03,  7.6203e-04, -4.7588e-03,\n",
       "          4.9506e-03, -7.2049e-03,  2.1396e-03, -3.7981e-04, -2.7731e-03,\n",
       "         -2.8983e-04,  1.5015e-02, -2.5850e-02,  1.0870e-02,  1.4125e-03,\n",
       "         -9.0017e-03,  5.4321e-03, -7.0946e-03, -6.6524e-03, -4.0747e-04,\n",
       "          4.2907e-03, -1.6650e-02,  6.1763e-03,  4.5040e-03,  6.6318e-03,\n",
       "         -1.6408e-02, -9.0291e-03, -2.5637e-04, -1.2321e-03,  9.6527e-03,\n",
       "          5.6799e-04,  3.8596e-03, -6.8554e-03, -1.7554e-04, -7.7480e-03,\n",
       "         -1.9190e-02, -5.3479e-03,  5.4230e-03, -2.2535e-03,  6.0320e-03,\n",
       "          1.3807e-02,  5.2258e-03,  1.6047e-02,  4.0910e-02, -5.8202e-03,\n",
       "         -1.3001e-02, -1.8711e-03, -1.1752e-02, -1.2366e-02, -5.0636e-03,\n",
       "          3.4010e-03, -1.4464e-02, -1.3218e-02, -6.2057e-03,  1.9009e-02,\n",
       "         -1.3285e-02, -1.3380e-02,  4.2315e-03, -6.0701e-05, -1.8504e-02,\n",
       "         -3.6390e-03,  1.8263e-02, -5.3379e-03, -1.4085e-02,  7.0778e-03,\n",
       "          1.7450e-02,  2.2154e-02,  5.4457e-03, -3.0039e-03, -3.1542e-03,\n",
       "          8.0197e-03, -1.7004e-02,  1.2076e-02,  9.9888e-03, -5.8664e-03,\n",
       "         -2.0733e-03,  6.8285e-03, -6.7051e-03, -6.1057e-03, -1.1112e-02,\n",
       "          8.4456e-03, -1.5570e-02,  6.2625e-03,  7.0650e-03, -3.4047e-02,\n",
       "         -1.1218e-02,  3.0334e-03, -7.6767e-03, -1.8960e-02,  1.5513e-02,\n",
       "          3.5970e-03, -1.5772e-02, -7.9872e-03], requires_grad=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.bert.encoder.layer[0].attention.self.value.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0149,  0.0202,  0.0302,  ...,  0.0057, -0.0113, -0.0225],\n",
      "        [-0.0569,  0.0186,  0.0062,  ...,  0.0121, -0.0153,  0.0126],\n",
      "        [ 0.0135, -0.0128,  0.0405,  ..., -0.0384,  0.0373, -0.0279],\n",
      "        ...,\n",
      "        [ 0.0521,  0.0021, -0.0508,  ...,  0.0534, -0.0321,  0.0261],\n",
      "        [ 0.0779,  0.0410,  0.0308,  ...,  0.0647,  0.0165,  0.0415],\n",
      "        [-0.0381, -0.0468,  0.0601,  ..., -0.0421, -0.0078, -0.0436]],\n",
      "       requires_grad=True)\n",
      "torch.Size([768, 768])\n",
      "---------para size------------\n",
      "768\n",
      "----------para[0]-----------\n",
      "tensor([ 1.4851e-02,  2.0216e-02,  3.0197e-02, -1.2674e-02, -4.5563e-02,\n",
      "        -1.0461e-02, -6.5851e-02, -4.9709e-03,  7.8242e-03, -3.6613e-02,\n",
      "        -6.8234e-03, -5.2694e-03,  7.2968e-03,  9.6949e-03, -1.7261e-02,\n",
      "         2.2025e-02, -2.8250e-02,  6.6066e-02, -2.3844e-02,  3.1265e-02,\n",
      "         8.7972e-03, -4.3364e-03,  1.7785e-02,  3.7758e-02,  1.5733e-02,\n",
      "        -2.1217e-02, -4.4960e-02,  8.0965e-03, -3.3140e-02, -3.7044e-02,\n",
      "        -4.5568e-02,  1.2841e-02, -1.6737e-02,  2.3717e-02,  9.7040e-03,\n",
      "        -5.5816e-03, -5.5093e-02, -3.0916e-02,  1.0236e-03,  4.0192e-02,\n",
      "        -2.5681e-02, -2.8045e-02, -1.8444e-02,  1.9184e-02, -2.5771e-02,\n",
      "         1.8481e-02,  1.6401e-03, -2.3128e-02, -2.3818e-03,  3.7178e-02,\n",
      "         5.0518e-03, -8.8737e-02,  2.4218e-02, -3.3128e-02, -2.1794e-02,\n",
      "         2.7451e-02, -4.7596e-02,  2.4857e-02, -2.0100e-03,  6.6045e-03,\n",
      "         4.7128e-02,  2.2003e-02,  5.8324e-03,  1.7321e-02, -2.1808e-02,\n",
      "         3.7206e-02, -1.2769e-03,  4.8089e-02,  1.7246e-02, -3.8268e-02,\n",
      "         1.5180e-03,  2.6074e-02, -4.1926e-02, -1.1396e-03,  4.5432e-02,\n",
      "        -3.1788e-02,  1.5981e-02, -9.6698e-04,  3.7242e-02,  1.2398e-02,\n",
      "         3.3414e-02,  5.0310e-02,  1.8017e-02,  2.4542e-02,  1.8162e-02,\n",
      "        -6.2750e-03,  1.6515e-02,  1.1230e-02, -1.5051e-02, -4.7246e-02,\n",
      "        -4.7666e-02,  2.7643e-02, -3.8573e-02,  2.8034e-02, -1.8731e-02,\n",
      "         6.2871e-02,  1.2296e-02, -2.2754e-04,  2.8232e-03, -1.9615e-02,\n",
      "        -2.4775e-02, -5.7789e-03, -8.0055e-03, -9.0184e-03, -8.5521e-02,\n",
      "        -5.4110e-03, -2.9582e-02,  1.4512e-03,  9.2377e-03, -5.8135e-02,\n",
      "        -1.5607e-02, -3.3880e-02,  2.5607e-04,  3.1179e-02, -2.6081e-02,\n",
      "         1.7989e-02, -2.4691e-02,  2.0437e-02, -5.9835e-02,  3.5262e-02,\n",
      "        -3.0806e-02, -3.8475e-03, -5.2349e-03, -6.3902e-02,  4.8474e-03,\n",
      "         3.4092e-02, -6.1529e-03, -4.4974e-03,  2.7305e-03,  9.5031e-03,\n",
      "         3.9209e-02, -2.1180e-03, -1.6584e-02, -1.7927e-02,  3.6593e-04,\n",
      "        -6.6471e-02,  7.1908e-03, -6.8403e-03, -5.9103e-04,  1.6131e-02,\n",
      "         2.3684e-02, -4.9418e-03,  1.3188e-02, -5.7923e-02,  1.6785e-02,\n",
      "        -3.9543e-02,  7.2014e-02, -2.0848e-02, -3.8654e-02,  5.7445e-02,\n",
      "         1.9464e-02, -3.3890e-04, -5.9528e-03,  1.4258e-02, -3.7832e-03,\n",
      "         6.0677e-02,  1.1464e-02,  7.4847e-04,  7.5176e-04, -6.4025e-02,\n",
      "         4.3484e-05,  2.3070e-03,  2.0776e-03, -1.0950e-02, -5.0137e-02,\n",
      "         5.0198e-02, -1.2035e-03, -2.4522e-02,  2.0771e-02,  2.3042e-02,\n",
      "        -2.3609e-02, -3.5536e-02,  5.4391e-03,  2.5640e-02,  5.3808e-03,\n",
      "         1.4210e-02,  5.9435e-03,  6.6145e-02,  5.5388e-02,  4.2902e-02,\n",
      "        -2.1968e-02,  4.5436e-02,  1.5989e-02,  3.6538e-02,  3.1299e-02,\n",
      "         1.7324e-02,  4.3253e-03, -2.9164e-02, -2.9216e-02, -5.3028e-02,\n",
      "         7.5142e-02,  3.5642e-03,  1.7507e-02, -3.9299e-03,  6.4976e-03,\n",
      "         1.2571e-02,  1.8300e-02, -3.1681e-02,  2.6041e-02, -2.3486e-02,\n",
      "         6.1402e-03,  4.0107e-02,  2.9332e-02, -2.2340e-02,  3.9818e-03,\n",
      "         1.4191e-03, -5.5703e-03, -2.5174e-02,  1.4104e-02, -3.8350e-02,\n",
      "        -3.4646e-02,  5.0956e-02,  8.7240e-03, -4.5553e-02,  4.1212e-03,\n",
      "         2.3767e-02,  2.9495e-02, -4.9050e-02, -4.2580e-03,  1.6738e-02,\n",
      "        -4.4084e-02, -6.0660e-03, -4.5966e-03, -1.4077e-02, -2.0349e-02,\n",
      "        -3.2602e-03, -2.7021e-02, -2.7311e-02,  1.1615e-02, -4.6755e-03,\n",
      "         2.2576e-02, -6.4603e-03,  7.0918e-03, -3.7289e-02, -1.9501e-02,\n",
      "         5.4455e-02,  1.9769e-02,  2.1865e-02, -3.7571e-02,  2.6659e-02,\n",
      "         2.1427e-03,  3.4944e-02,  4.5756e-03, -5.5476e-02, -4.2464e-02,\n",
      "         5.3932e-02,  1.1381e-02, -3.1778e-02, -6.9258e-03,  3.2454e-02,\n",
      "         2.7876e-02, -2.4537e-02,  5.0015e-02,  3.7444e-02, -1.3746e-02,\n",
      "        -4.2695e-03,  1.0982e-02,  3.6336e-03, -1.2091e-02,  2.0865e-02,\n",
      "        -4.8984e-02,  2.2773e-02,  3.8664e-02,  1.7473e-02, -7.2470e-03,\n",
      "        -4.9117e-02, -4.3495e-03,  6.4758e-02,  2.3064e-03,  8.4164e-03,\n",
      "        -5.5247e-03, -2.4763e-02, -1.7614e-02,  4.9069e-02, -1.8991e-02,\n",
      "         5.0956e-02, -2.7864e-02,  3.2756e-02, -6.9535e-03,  3.9751e-02,\n",
      "        -1.5658e-02,  2.4953e-02, -3.1209e-02, -1.0565e-02,  1.6564e-02,\n",
      "         4.9748e-02,  1.3217e-02, -5.5622e-03,  3.0219e-02, -1.2242e-02,\n",
      "        -2.4969e-02,  1.1580e-02, -3.5446e-02,  2.6528e-02, -1.5968e-02,\n",
      "        -2.0235e-02,  6.8974e-03,  4.6123e-02,  1.0826e-02, -4.4908e-02,\n",
      "         1.7301e-02, -2.0477e-02, -1.9311e-03, -5.1473e-02,  1.9843e-02,\n",
      "        -8.3545e-03,  3.1054e-02, -1.4288e-02, -3.6785e-02,  2.6121e-02,\n",
      "        -1.5958e-02,  9.7386e-03,  6.5044e-03, -1.3176e-02,  9.4455e-03,\n",
      "        -5.8203e-02, -3.0832e-02, -9.7803e-04,  1.2575e-02, -1.2697e-03,\n",
      "         1.7105e-02,  1.5729e-02,  1.2890e-02,  7.4535e-03, -3.2947e-02,\n",
      "         3.0642e-02, -8.4294e-02,  4.3380e-02,  1.6838e-02, -3.9152e-02,\n",
      "        -1.2205e-02, -3.3415e-02,  1.3148e-02,  3.4440e-02, -3.3280e-03,\n",
      "        -1.0203e-02, -1.4382e-02,  1.2375e-02,  6.3415e-02,  2.6683e-02,\n",
      "        -1.5321e-02,  7.8862e-02,  5.5897e-02,  2.5294e-03, -3.6036e-02,\n",
      "         1.0948e-02, -1.6969e-02, -8.8956e-04, -8.4491e-03, -3.1120e-03,\n",
      "         7.3606e-02,  7.9226e-03,  2.0676e-02, -2.0598e-03, -2.4322e-02,\n",
      "        -5.0109e-02, -2.9210e-02, -1.0328e-02, -2.0108e-02, -3.6871e-02,\n",
      "         1.7692e-02, -2.6296e-02, -5.1828e-02,  2.5282e-02, -2.6372e-02,\n",
      "        -3.6896e-02,  1.5794e-02, -1.8982e-02, -4.4979e-02,  2.8802e-02,\n",
      "         6.7762e-02,  4.4498e-02,  1.8181e-02,  6.8217e-03, -1.0013e-01,\n",
      "        -3.1993e-02,  1.7043e-02, -2.5340e-02, -2.9821e-02,  3.2002e-02,\n",
      "        -3.4952e-02, -1.6272e-02, -2.0474e-02,  7.5115e-03, -1.4647e-02,\n",
      "         9.1258e-03,  1.8855e-02,  1.6453e-02, -3.0609e-02, -7.1924e-03,\n",
      "        -5.5729e-02,  6.2566e-04, -9.2370e-03, -4.6174e-02, -2.5524e-04,\n",
      "         2.0515e-02, -2.0948e-02,  1.7863e-02, -1.7545e-02, -4.7375e-02,\n",
      "        -1.3542e-02, -4.0338e-02, -1.2890e-02,  4.3042e-02,  4.9049e-02,\n",
      "         3.8965e-02, -2.7959e-02,  3.1108e-02, -2.1615e-02,  3.8336e-02,\n",
      "        -1.8648e-02,  6.4724e-02,  2.8678e-02,  3.4326e-02, -4.2610e-02,\n",
      "        -2.0485e-02, -2.6241e-02,  7.2497e-03, -9.4337e-03,  2.9387e-02,\n",
      "         4.5349e-02, -8.3902e-03, -2.5698e-02, -2.5572e-03,  3.0825e-03,\n",
      "        -2.2366e-02,  6.8279e-02, -5.0771e-02,  1.9694e-02, -1.9320e-02,\n",
      "         5.2231e-02,  2.3700e-02, -2.1295e-02,  1.8061e-02, -4.4301e-02,\n",
      "        -5.5799e-02, -2.9638e-02, -4.6190e-06, -3.3221e-02, -5.2424e-02,\n",
      "         2.6401e-02, -1.0922e-03,  1.0063e-02, -2.0552e-02,  1.6837e-02,\n",
      "         2.7405e-02, -4.7541e-04, -4.2423e-03, -1.4707e-02, -1.6334e-02,\n",
      "         9.5111e-03,  3.5740e-03,  3.9017e-03,  3.1840e-03,  9.9065e-03,\n",
      "        -3.7111e-03, -1.2459e-02,  3.9904e-04,  2.3849e-02,  2.9521e-02,\n",
      "         2.8385e-02,  1.7063e-02, -5.5301e-03, -2.8832e-02, -4.9048e-02,\n",
      "         5.3324e-04,  2.0068e-02,  4.4340e-02, -2.6319e-02,  3.1841e-02,\n",
      "         1.7769e-02, -3.2400e-03, -1.7521e-02,  3.5848e-02,  5.1874e-02,\n",
      "         5.5446e-02, -2.4933e-02, -1.1251e-02,  5.7852e-02,  2.6918e-02,\n",
      "        -9.7930e-03,  3.9964e-03, -1.8291e-02,  3.7394e-02, -2.4975e-02,\n",
      "         5.7293e-02, -4.0160e-04, -2.7649e-02, -1.6239e-02,  6.3503e-02,\n",
      "        -1.8197e-02, -5.3521e-03,  5.7360e-04, -6.6118e-02, -5.1744e-03,\n",
      "         5.8258e-02,  3.7535e-02,  4.8271e-03,  1.4721e-02, -9.1464e-03,\n",
      "         6.0415e-02, -3.4202e-02,  2.9504e-02,  1.4000e-02, -6.1196e-03,\n",
      "        -3.7491e-02, -2.8383e-03,  1.5472e-02, -2.6891e-02, -3.8997e-02,\n",
      "        -2.7617e-02, -1.6515e-03, -7.4968e-03, -1.2792e-02, -2.5102e-02,\n",
      "         1.0268e-02,  9.8405e-03, -7.1261e-03, -1.2668e-02,  8.8514e-03,\n",
      "         3.4447e-02,  6.5505e-02,  2.0754e-02, -1.0634e-02,  3.2487e-02,\n",
      "        -1.0331e-04, -3.5552e-02,  5.9431e-03,  2.9349e-02,  1.8191e-02,\n",
      "        -9.7524e-03,  7.9768e-03, -2.0501e-02,  3.4824e-02, -2.5137e-03,\n",
      "        -1.6539e-02, -8.5725e-03,  5.4845e-03,  1.6445e-02,  7.2590e-03,\n",
      "        -3.4352e-02, -5.1315e-02, -2.3102e-02, -3.0382e-03,  3.2683e-02,\n",
      "        -9.8279e-03, -6.9754e-02, -3.7873e-03,  2.7018e-02,  1.3441e-02,\n",
      "        -1.9393e-02,  2.8203e-02, -1.3521e-02,  2.7139e-02, -1.8666e-02,\n",
      "         3.0891e-02, -1.5170e-02, -1.9091e-02,  1.0661e-02,  1.1084e-02,\n",
      "        -1.1414e-02, -4.4288e-03, -3.1370e-02, -1.3790e-02, -4.1780e-02,\n",
      "        -2.6312e-03,  2.9298e-03,  1.1541e-02,  1.4438e-02,  2.8238e-02,\n",
      "        -6.4754e-03,  4.6923e-02,  2.0852e-02,  3.9931e-04,  3.7443e-02,\n",
      "        -2.7437e-02,  1.0393e-03,  3.2322e-03,  2.9743e-02,  9.6277e-03,\n",
      "        -5.5478e-03, -6.4951e-02,  3.0828e-03, -1.3232e-02,  5.5176e-03,\n",
      "         5.5171e-03, -1.7996e-02, -7.4582e-03,  2.7623e-02,  2.2680e-02,\n",
      "         9.6706e-03,  1.3876e-02,  1.8219e-02,  4.1438e-02, -6.4947e-03,\n",
      "         2.0344e-02,  3.7689e-02, -3.8288e-03, -4.5313e-02, -3.8836e-02,\n",
      "         3.8151e-02, -1.4953e-02,  2.4721e-02,  4.9033e-02,  3.1725e-02,\n",
      "        -5.2025e-02,  7.1664e-02, -8.4911e-02,  3.0026e-02,  2.1917e-02,\n",
      "        -2.0910e-02, -3.4123e-02,  1.9029e-02,  2.9433e-02, -1.0630e-02,\n",
      "         7.2122e-05,  2.5691e-02,  4.1953e-02, -2.8498e-02, -4.7863e-02,\n",
      "         2.9210e-02, -3.6651e-02, -1.8755e-02,  2.6117e-02,  1.3878e-03,\n",
      "         2.0683e-02, -5.6518e-03,  1.0923e-02, -1.8961e-02,  2.0067e-02,\n",
      "        -2.1499e-02,  3.3347e-02, -2.3990e-02,  1.0266e-02,  2.0912e-02,\n",
      "        -4.8698e-02, -6.9346e-03,  1.1722e-02,  4.0578e-02,  3.9684e-02,\n",
      "         3.2582e-02, -4.3327e-02,  1.0632e-02,  2.7634e-02,  1.1026e-02,\n",
      "         4.4937e-02,  2.4876e-02,  2.4760e-02, -3.8011e-03,  9.7441e-04,\n",
      "        -1.8182e-03,  1.3098e-02, -8.3939e-03, -2.6304e-03, -1.4085e-02,\n",
      "        -6.9456e-03,  4.1951e-02, -4.7652e-02, -2.1248e-02,  4.0842e-02,\n",
      "        -2.1211e-02,  2.3125e-02, -1.3785e-02, -1.1102e-02,  3.4494e-03,\n",
      "         6.5672e-02, -5.6329e-02, -3.0162e-02,  4.7204e-03,  1.5682e-03,\n",
      "        -1.1310e-02, -2.2277e-02,  2.2692e-02,  3.2892e-02,  4.4387e-02,\n",
      "        -1.3158e-02,  4.0206e-03, -1.8950e-03, -1.3512e-02,  4.1891e-02,\n",
      "         6.7922e-02, -2.2966e-02, -1.2948e-02,  2.5031e-02,  3.6590e-03,\n",
      "         4.5703e-02, -2.5248e-03, -2.2510e-02, -6.4051e-02,  6.1430e-02,\n",
      "        -3.7559e-02, -2.9319e-02,  6.9745e-03, -4.2952e-02, -3.3122e-02,\n",
      "         3.2298e-02,  2.0021e-02,  4.1416e-02, -1.7219e-02,  2.0098e-02,\n",
      "         1.2007e-02, -3.7280e-02, -4.5020e-02,  1.1040e-02, -4.4277e-03,\n",
      "        -2.4644e-02, -6.9732e-02, -1.7082e-02,  2.9797e-02,  5.4714e-02,\n",
      "        -9.1764e-03,  5.8022e-03, -2.3022e-02,  3.1242e-03, -8.6925e-05,\n",
      "         2.2435e-02, -6.9439e-03, -3.0391e-02, -1.7002e-02, -4.0043e-02,\n",
      "         2.2902e-02, -1.7748e-02, -2.2542e-02, -3.6216e-02,  1.3651e-02,\n",
      "         1.2556e-02, -4.3197e-02,  2.3573e-03,  9.2059e-03, -4.7778e-03,\n",
      "        -8.2444e-03, -3.5257e-02, -1.3385e-02,  2.7957e-02, -4.0815e-02,\n",
      "        -5.9548e-03,  6.9473e-02, -1.0060e-02, -1.5202e-02, -1.1246e-03,\n",
      "         3.2562e-02, -2.4690e-02,  3.9340e-02,  3.3442e-02,  2.4800e-02,\n",
      "         9.3165e-03, -7.9833e-02, -2.3113e-02,  2.7919e-02,  6.2275e-03,\n",
      "         1.8700e-02,  1.5030e-02, -4.1287e-02,  1.8008e-02, -7.7392e-04,\n",
      "         4.5066e-02,  3.7365e-02,  7.6024e-02, -7.6272e-03,  3.1678e-02,\n",
      "         6.0344e-03, -4.4109e-02,  3.9929e-02,  2.9965e-02, -3.2416e-02,\n",
      "         5.6851e-03, -1.1319e-02, -2.2535e-02], grad_fn=<SelectBackward>)\n",
      "----------para[0] size-----------\n",
      "torch.Size([768])\n",
      "Parameter containing:\n",
      "tensor([-5.4716e-02, -7.7908e-02,  6.8522e-02, -7.4388e-02, -1.5325e-02,\n",
      "        -1.1723e-01, -1.0125e-01, -1.4806e-01,  8.1477e-03, -1.7296e-02,\n",
      "         1.8127e-01,  7.6276e-02,  2.6111e-01, -6.9813e-02, -6.7967e-02,\n",
      "        -6.6124e-02, -1.2676e-02,  3.4312e-02, -1.2856e-02, -1.2916e-01,\n",
      "        -3.9252e-02,  4.2544e-02,  3.2868e-02,  1.1273e-02,  5.6740e-02,\n",
      "         3.7373e-02,  2.7498e-01,  1.8963e-01, -7.8654e-02,  9.5707e-02,\n",
      "        -5.6606e-02,  7.8172e-02,  3.1357e-02, -4.8359e-03,  1.7712e-01,\n",
      "         3.4018e-02,  8.4428e-02,  1.2051e-01, -1.2274e-01,  9.3144e-03,\n",
      "         1.1228e-01,  2.0177e-02,  1.5401e-01, -4.6656e-02, -8.8206e-03,\n",
      "         1.4908e-03,  8.4583e-03,  2.6128e-01,  1.3035e-02, -1.1177e-01,\n",
      "         1.0067e-01, -9.7890e-02, -5.9544e-02,  2.1643e-01, -5.2873e-02,\n",
      "         2.7767e-02,  2.1501e-01, -2.0090e-01, -5.2822e-02, -4.4415e-02,\n",
      "        -1.1083e-01,  1.2788e-01,  2.7844e-02,  1.1834e-01,  9.4454e-02,\n",
      "         1.5535e-01, -2.4485e-02,  1.2660e-01,  6.2933e-02, -5.9563e-02,\n",
      "        -2.2441e-02, -3.5520e-02, -2.0515e-01,  1.2356e-01, -7.6458e-03,\n",
      "        -8.0719e-03, -9.1381e-02,  2.9600e-02, -1.4259e-01, -9.8588e-02,\n",
      "        -1.3494e-01, -2.3689e-02, -5.1790e-02, -8.1778e-02, -1.0818e-01,\n",
      "        -6.0774e-02,  6.4156e-02, -9.0860e-02,  2.2681e-01, -5.5272e-02,\n",
      "         2.6583e-02, -2.1861e-01,  5.2028e-02,  1.5395e-01,  6.4367e-02,\n",
      "        -5.0033e-02,  2.0827e-02,  5.6589e-02, -1.5144e-01, -2.2251e-02,\n",
      "         3.6736e-02,  6.6299e-02,  4.8167e-02, -5.1961e-02, -7.5416e-02,\n",
      "         2.5297e-02,  3.0426e-02, -1.1449e-02, -8.1716e-02, -1.6980e-01,\n",
      "         1.5418e-02,  5.8772e-02,  2.2698e-02, -6.1444e-02, -1.2255e-01,\n",
      "        -9.5890e-02,  2.4028e-02,  1.0414e-01, -9.6488e-03,  1.6134e-01,\n",
      "         8.4602e-02,  4.7666e-02,  1.8891e-02, -1.9574e-01, -1.3921e-02,\n",
      "         1.6440e-01,  1.6261e-01,  4.3704e-02,  9.0670e-02, -1.0835e-01,\n",
      "        -9.7826e-02, -1.3610e-01, -6.1978e-02,  2.2427e-01,  5.5531e-03,\n",
      "        -4.2779e-02,  1.9470e-02,  6.2738e-02,  9.3982e-03, -1.2984e-01,\n",
      "        -6.8327e-02,  2.7087e-02,  5.8160e-03,  9.1933e-02, -8.3500e-02,\n",
      "        -1.3804e-02,  3.3995e-02,  1.0113e-01,  6.9083e-02, -1.3150e-01,\n",
      "         5.5834e-02,  9.5001e-02,  4.9433e-02,  1.0624e-01,  3.9033e-02,\n",
      "        -1.4748e-01,  1.9100e-01,  1.6272e-01,  9.7632e-02,  3.7702e-02,\n",
      "         6.6951e-02,  1.4659e-01, -1.2083e-01,  2.2025e-01, -1.5808e-02,\n",
      "         7.4403e-02,  2.1921e-01,  3.8873e-02,  2.1651e-01,  3.7497e-03,\n",
      "         3.0900e-02,  5.6761e-02,  1.9668e-01,  2.7528e-02,  3.4923e-02,\n",
      "         1.7181e-01, -4.2738e-02,  2.8616e-02,  2.3104e-01,  4.7568e-02,\n",
      "         5.4537e-02, -6.4804e-02, -6.1896e-03, -1.8745e-02,  3.7760e-02,\n",
      "        -6.6188e-02, -2.4636e-01,  2.8327e-02, -1.4721e-01,  6.5707e-02,\n",
      "        -6.0335e-02,  9.4139e-02, -2.9727e-01,  2.5435e-01,  6.5519e-02,\n",
      "        -1.4907e-01, -1.6308e-01,  4.1241e-01, -4.0129e-01,  6.2064e-02,\n",
      "         7.9119e-02,  1.3739e-01,  2.4701e-01,  1.4516e-01,  9.1196e-02,\n",
      "        -9.7494e-02,  1.0116e-01, -2.7261e-03,  4.7459e-02,  2.8604e-02,\n",
      "         2.1046e-01, -3.1043e-01,  2.0516e-01, -1.7438e-01, -1.5192e-01,\n",
      "        -2.8603e-01, -1.8623e-01,  2.7630e-01,  2.0834e-01,  6.6856e-02,\n",
      "         2.9404e-01,  2.9672e-01,  3.6719e-01, -6.8198e-04, -5.7973e-02,\n",
      "         1.3354e-01,  1.2507e-01, -1.1922e-01, -5.8016e-03,  2.1161e-01,\n",
      "         2.5305e-01,  2.1150e-01, -3.8970e-01,  1.0456e-01, -4.2923e-01,\n",
      "         6.9521e-02,  9.5228e-02,  1.8491e-01,  1.4188e-02, -1.4583e-01,\n",
      "        -4.5271e-02,  2.5165e-02, -3.4472e-02, -2.4899e-01,  5.5791e-03,\n",
      "        -1.1418e-01,  5.9325e-02, -4.7543e-01,  1.3054e-02, -1.5630e-01,\n",
      "         3.3867e-01,  4.1547e-01,  5.0879e-03, -6.3650e-02, -2.4425e-01,\n",
      "        -3.5105e-01, -1.4307e-01,  3.8655e-02,  9.3640e-02,  5.6189e-02,\n",
      "         6.6135e-02,  1.1448e-02, -3.0619e-02,  1.9858e-02,  4.1786e-02,\n",
      "         1.8650e-02, -3.2012e-02,  7.9369e-02, -7.1955e-02,  9.2480e-02,\n",
      "        -1.5541e-01,  7.9153e-02, -7.8439e-02,  1.1106e-01,  6.4411e-02,\n",
      "         1.4200e-01,  2.6566e-01,  5.5827e-02,  1.4658e-02, -4.4276e-02,\n",
      "        -3.4003e-02,  2.0168e-02, -6.4465e-02, -2.7897e-01, -4.6690e-02,\n",
      "         1.0271e-01,  5.1541e-02, -2.4742e-01,  6.6788e-02, -7.4638e-02,\n",
      "         1.8802e-02,  8.4628e-02,  6.1641e-02, -5.7920e-02, -3.1358e-02,\n",
      "        -7.7616e-02,  1.0588e-01, -8.9888e-02, -3.8865e-02,  3.0453e-02,\n",
      "        -9.7556e-03, -4.0980e-02,  2.7695e-03, -8.4009e-03,  1.0038e-01,\n",
      "         9.0062e-03, -1.6214e-02, -7.2739e-02,  8.8177e-02,  1.3570e-01,\n",
      "         2.3396e-02,  4.8222e-02, -1.2075e-01,  4.0286e-02,  1.9641e-02,\n",
      "        -6.9646e-03, -1.9248e-02, -4.0954e-02, -1.3672e-01, -6.5254e-02,\n",
      "        -3.0771e-02, -1.2403e-01, -1.1838e-02, -2.5131e-01, -1.0494e-01,\n",
      "         7.0608e-02, -1.3485e-01, -7.3288e-02,  2.8042e-01, -1.3026e-01,\n",
      "        -1.3177e-01, -1.7446e-01,  1.4218e-01, -1.6909e-01,  7.8403e-04,\n",
      "         6.4576e-02, -2.1656e-02,  8.3174e-02, -2.9271e-01,  2.1639e-01,\n",
      "        -1.6234e-01,  1.1961e-01, -1.2575e-01, -3.3262e-02, -9.2886e-02,\n",
      "         5.0906e-02,  1.6752e-01,  1.6012e-01, -4.5262e-02,  1.4870e-01,\n",
      "         1.4851e-01,  1.4739e-01,  8.5201e-02, -9.0774e-02, -1.2238e-01,\n",
      "         3.1675e-01, -1.6212e-01, -3.2008e-01, -1.7403e-01,  1.3978e-01,\n",
      "         1.8175e-01,  1.3683e-01, -1.3472e-01,  6.7134e-02, -2.6356e-01,\n",
      "        -1.0858e-01,  6.5107e-02,  2.5688e-01, -6.9935e-02,  7.8644e-02,\n",
      "        -1.2431e-01,  2.2616e-01,  9.8527e-02, -1.9902e-01,  5.6092e-02,\n",
      "        -8.3141e-02, -1.8455e-01,  7.9806e-02,  1.1138e-01, -5.6300e-02,\n",
      "         2.1398e-01, -4.0955e-02,  1.3219e-01,  1.0220e-01, -4.4698e-02,\n",
      "         1.7768e-02,  9.1315e-02,  6.0929e-02, -1.3839e-02, -4.1985e-02,\n",
      "         4.3797e-02, -7.4853e-02, -1.1323e-02, -2.2031e-01, -1.0183e-01,\n",
      "         1.2712e-01, -1.3242e-01, -1.1565e-01,  1.8595e-01, -1.8167e-02,\n",
      "         1.1078e-01, -1.1727e-01, -6.2769e-02, -1.2046e-01, -1.1143e-01,\n",
      "        -3.0830e-02,  9.4773e-03,  6.2293e-02,  5.0068e-02,  8.1740e-03,\n",
      "        -1.5686e-01, -1.4442e-01, -2.7185e-02,  8.1537e-02,  7.9163e-02,\n",
      "         4.0840e-03, -2.1487e-01,  9.5343e-02, -1.5104e-01, -4.4333e-02,\n",
      "         1.1102e-01,  6.9637e-02,  1.4478e-01,  8.1832e-02, -9.2866e-02,\n",
      "        -6.3423e-02,  3.7702e-02,  4.5825e-02, -1.4801e-01, -1.3922e-01,\n",
      "        -9.0842e-02, -6.2491e-03,  6.4611e-02,  1.0812e-01,  2.3729e-02,\n",
      "         6.5798e-03, -5.3267e-02, -1.6400e-01, -2.7280e-02, -3.0661e-02,\n",
      "         3.4069e-02,  5.6358e-02,  1.2963e-01, -3.6969e-02, -1.2156e-02,\n",
      "         6.5373e-02,  1.1456e-02, -5.6320e-02,  3.5257e-02,  1.1526e-01,\n",
      "         9.6198e-03, -3.7374e-02,  3.4761e-02,  6.0967e-02,  3.1305e-02,\n",
      "         9.2000e-02, -9.7943e-02, -5.8522e-02,  7.5684e-04, -9.1407e-02,\n",
      "         1.9529e-02,  1.5313e-01,  2.5675e-02,  1.9614e-01, -1.3480e-01,\n",
      "         5.5200e-03,  6.1979e-02, -4.3497e-02,  7.6635e-02, -8.3419e-03,\n",
      "         1.3345e-01, -1.5119e-01, -3.9470e-02,  1.3573e-01,  1.0002e-01,\n",
      "         1.5475e-01,  5.7226e-02, -1.9240e-01, -7.3686e-02, -4.3737e-02,\n",
      "         7.1305e-02,  2.1758e-02, -7.2643e-02,  1.7253e-02, -3.0271e-02,\n",
      "        -2.0221e-01, -7.1486e-02, -2.9186e-01,  1.5044e-01,  1.5583e-01,\n",
      "         2.9893e-02,  4.8762e-02,  6.4654e-02,  7.0698e-02,  3.3167e-02,\n",
      "        -1.1418e-01,  5.0647e-02,  7.2718e-02,  9.9397e-02, -1.2620e-01,\n",
      "        -1.6164e-01, -2.6738e-01,  2.3031e-01, -1.2166e-01,  9.8823e-02,\n",
      "        -1.1875e-01,  2.3339e-02, -4.9492e-02,  1.3847e-01,  1.5516e-01,\n",
      "         2.4421e-02,  3.5754e-02, -3.6315e-02, -2.0388e-02, -1.1485e-02,\n",
      "        -3.4140e-02,  4.7055e-02, -4.0044e-02,  1.8306e-01, -1.2034e-01,\n",
      "        -2.0475e-01,  1.0126e-01,  3.2808e-02, -1.7707e-01,  1.6958e-01,\n",
      "         1.0857e-01,  1.1150e-01, -8.3792e-02, -5.4626e-02,  5.0060e-02,\n",
      "        -6.7255e-02, -1.3386e-01, -1.1687e-02, -1.2649e-01,  9.0255e-02,\n",
      "        -1.5605e-02, -4.9649e-02, -3.8308e-02, -4.6496e-02,  7.3953e-02,\n",
      "        -1.3116e-02,  7.2230e-02,  1.1686e-01, -2.4297e-02, -1.4851e-01,\n",
      "        -4.0488e-02,  5.9836e-02,  5.4655e-02,  1.8690e-02,  1.2686e-02,\n",
      "         4.5762e-02,  2.8808e-02, -8.2282e-03,  1.9355e-03,  2.3796e-02,\n",
      "         5.6526e-02, -5.1990e-02, -1.4947e-02,  8.8311e-02,  1.2986e-01,\n",
      "        -4.4549e-02,  1.1724e-01, -1.7325e-01,  2.3886e-01,  1.7822e-01,\n",
      "         4.8517e-02,  6.1643e-02, -5.9189e-02, -1.8967e-03,  7.0344e-02,\n",
      "        -3.5617e-02,  3.8430e-02,  3.7554e-02,  9.9080e-02, -9.8629e-02,\n",
      "         1.3150e-02, -8.0413e-02, -4.8156e-02,  2.9464e-02,  3.8491e-02,\n",
      "         9.4823e-02,  1.4321e-01,  6.5820e-02,  1.0741e-01,  8.3322e-02,\n",
      "        -6.2829e-03, -1.1482e-01,  1.1046e-01,  4.6113e-02, -1.0736e-01,\n",
      "         9.3267e-02, -1.0471e-01, -8.9781e-05, -1.3229e-01, -1.0284e-01,\n",
      "        -3.9668e-02, -7.2092e-02,  1.9191e-01,  2.5932e-02, -1.1277e-02,\n",
      "         3.2755e-02,  3.0483e-02,  1.8232e-01, -8.7429e-04, -2.2763e-01,\n",
      "        -4.7979e-02, -1.2466e-01,  6.8251e-02,  7.4725e-02,  1.8499e-01,\n",
      "        -8.3638e-02,  1.1825e-01, -7.5022e-02, -2.4642e-01,  3.2591e-01,\n",
      "         6.0861e-02, -1.6526e-01, -1.5493e-02,  1.3068e-01, -6.9744e-02,\n",
      "         3.9338e-02, -1.1888e-01,  5.6315e-02,  6.1962e-02, -1.6590e-02,\n",
      "        -3.4224e-02,  3.6132e-02,  9.6250e-02,  1.0737e-01, -1.9450e-02,\n",
      "         1.6059e-02,  6.0217e-02, -2.0884e-01, -9.4228e-02, -2.2700e-01,\n",
      "        -6.8012e-02, -4.5394e-02, -2.0906e-01, -8.1361e-02,  1.4911e-01,\n",
      "        -5.4860e-02,  9.5052e-02,  1.5166e-01, -1.0640e-01,  1.3696e-01,\n",
      "        -4.1039e-02,  1.4286e-01, -5.5490e-02, -7.7748e-02,  3.6754e-02,\n",
      "        -2.8352e-03,  2.5856e-01,  1.1507e-01,  5.4708e-03, -1.3976e-01,\n",
      "        -1.3829e-02, -7.1658e-03, -1.0035e-01, -1.2364e-03,  2.2421e-01,\n",
      "         7.6350e-02, -2.5048e-02,  2.3243e-01, -9.1405e-02, -1.8843e-01,\n",
      "        -1.6159e-02, -1.5906e-01,  4.9696e-02,  1.6219e-02,  1.2416e-02,\n",
      "         2.6411e-02, -8.7986e-02, -3.5826e-02, -5.0790e-02, -1.8190e-01,\n",
      "         2.5015e-02, -2.4837e-02,  5.7420e-02, -9.2130e-02, -1.5600e-01,\n",
      "         3.7253e-02, -5.8690e-02,  9.8427e-02,  3.5033e-02,  4.1556e-02,\n",
      "        -1.0706e-01,  1.4577e-01,  5.7381e-02,  1.0353e-02, -8.7904e-02,\n",
      "         1.5400e-02,  8.5139e-02, -1.1584e-01,  7.1426e-02,  7.6077e-02,\n",
      "         4.7541e-02,  2.5059e-02, -2.1360e-01,  8.4217e-02,  6.5652e-02,\n",
      "         1.2667e-02,  1.6117e-01,  6.9184e-02,  2.2700e-02, -1.0364e-01,\n",
      "         2.8618e-02,  1.3722e-01,  9.0126e-03, -1.0229e-01,  2.6236e-01,\n",
      "        -1.7732e-01, -6.3850e-02, -3.7044e-01, -2.3868e-01,  8.2769e-02,\n",
      "         1.2170e-01, -1.1457e-01, -1.0987e-01,  7.4606e-04, -1.3333e-01,\n",
      "         3.7076e-01,  5.0393e-02,  2.4277e-01,  7.1339e-02,  1.4986e-01,\n",
      "         1.3536e-01, -3.5142e-02, -5.3316e-02, -4.6415e-02,  3.2747e-02,\n",
      "         6.8823e-02,  4.0183e-02,  2.1011e-02,  9.3121e-02, -5.6389e-02,\n",
      "         3.8018e-02,  2.1728e-01, -3.0388e-02,  1.0193e-01, -2.2396e-01,\n",
      "         6.6602e-02, -2.7955e-02, -1.4911e-01,  2.9827e-01, -2.1355e-01,\n",
      "         2.3449e-01,  1.2606e-01, -5.1404e-03,  4.5069e-02, -7.8395e-02,\n",
      "         2.8660e-01, -2.9758e-01,  1.9904e-01, -6.3779e-02,  1.1295e-01,\n",
      "         9.1710e-02,  3.5734e-02,  1.5043e-02, -3.3492e-01, -3.1777e-01,\n",
      "        -6.1539e-02,  1.0700e-01, -2.2775e-01, -8.8156e-02,  3.0067e-01,\n",
      "        -1.7568e-01,  6.0592e-02, -9.4741e-03], requires_grad=True)\n",
      "torch.Size([768])\n",
      "---------para size------------\n",
      "768\n",
      "----------para[0]-----------\n",
      "tensor(-0.0547, grad_fn=<SelectBackward>)\n",
      "----------para[0] size-----------\n",
      "torch.Size([])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for para in model.bert.encoder.layer[0].attention.self.query.parameters():\n",
    "    print(para)\n",
    "    print(para.size())\n",
    "    print('---------para size------------')\n",
    "    print(len(para))\n",
    "    print('----------para[0]-----------')\n",
    "    print(para[0])\n",
    "    print('----------para[0] size-----------')\n",
    "    print(para[0].size())\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0149,  0.0202,  0.0302,  ...,  0.0057, -0.0113, -0.0225],\n",
       "        [-0.0569,  0.0186,  0.0062,  ...,  0.0121, -0.0153,  0.0126],\n",
       "        [ 0.0135, -0.0128,  0.0405,  ..., -0.0384,  0.0373, -0.0279],\n",
       "        ...,\n",
       "        [ 0.0521,  0.0021, -0.0508,  ...,  0.0534, -0.0321,  0.0261],\n",
       "        [ 0.0779,  0.0410,  0.0308,  ...,  0.0647,  0.0165,  0.0415],\n",
       "        [-0.0381, -0.0468,  0.0601,  ..., -0.0421, -0.0078, -0.0436]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(model.bert.encoder.layer[0].attention.self.query.parameters())[0].data\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0018,  0.0177,  0.0639,  ..., -0.0090, -0.0084,  0.0235],\n",
       "        [ 0.0073,  0.0067,  0.0388,  ...,  0.0165, -0.0252,  0.0192],\n",
       "        [ 0.0254, -0.0230, -0.0188,  ..., -0.0064, -0.0064,  0.0853],\n",
       "        ...,\n",
       "        [-0.0181,  0.0045,  0.0045,  ...,  0.0032, -0.0152, -0.0131],\n",
       "        [-0.0080, -0.0160, -0.0021,  ..., -0.0290,  0.0550,  0.0239],\n",
       "        [-0.0355, -0.0506, -0.0188,  ...,  0.0300, -0.0216,  0.0226]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(model.bert.encoder.layer[1].attention.self.query.parameters())[0].data\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0241,  0.0389,  0.0040,  ...,  0.0335,  0.0147, -0.0311],\n",
       "        [-0.0576, -0.0329, -0.0015,  ..., -0.0170, -0.0157,  0.0442],\n",
       "        [-0.0223,  0.0025, -0.0002,  ..., -0.0745,  0.0420,  0.0251],\n",
       "        ...,\n",
       "        [-0.0032, -0.0407, -0.0129,  ..., -0.0144, -0.0147, -0.0322],\n",
       "        [-0.0037, -0.0208, -0.0348,  ...,  0.0338,  0.0078,  0.0418],\n",
       "        [-0.0099,  0.0293, -0.0044,  ..., -0.0352, -0.0309, -0.0080]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = list(model.bert.encoder.layer[2].attention.self.query.parameters())[0].data\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9463)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(x,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0797)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(x,y_hat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
